{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "We want better estimate of $\\bar{C}_{ij} = p(w_2 = j | w_1 = i) = \\frac{C_{ij}}{p(w_1 = i)}$, the  probability of seeing $j$ conditional on word $i$. We can get good estimate of $p(w_1 = i)$, so we need to improve $C_{ij} = p(w_1 = i, w_2 = j)$, the co-occurrence probability for word pair $(i, j)$. \n",
    "\n",
    "\n",
    "## Empirical Estimate\n",
    "\n",
    "Given document $d$, whith $(n^d, n^d_i, n^d_j)$ being (# of total word counts, # of word $i$, # of word $j$). The empirical estimate is:\n",
    "\n",
    "\\begin{align}\n",
    "    \\hat{C}^d_{ij} &= \\frac{n^d_i n^d_j}{n^d (n^d - 1)} \\approx \\hat{\\pi}^d_i \\hat{\\pi}^d_j\\\\\n",
    "    \\hat{\\pi}^d_i & = \\frac{n^d_i}{n^d}\n",
    "\\end{align}\n",
    "\n",
    "Then $\\hat{C}_{ij} = \\frac{1}{D}\\sum_d \\hat{C}^d_{ij}$ is averaging estimates over all $D$ documents.\n",
    "\n",
    "### Issues\n",
    "\n",
    "The estimate is poor for many in-frequent words. Since in the next step we treat each row of $\\bar{C}$ as a point in high-dimension and find extramal point among the rows, the poor estimate of those entries in $\\bar{C}$ cause severe problems.\n",
    "\n",
    "\n",
    "## A Simpler Model\n",
    "\n",
    "In Empirical estimate we can see estimating of $\\pi^d_i$ is key. We mainly want to shrink those in-frequent words so that their $\\bar{C}$ rows don't stand out. Below I disregard the weak dependency among multinomial counts and treat each as generated from binomial: \n",
    "\n",
    "\\begin{align}\n",
    "    & n^d_i \\sim \\text{Bin}(n^d, \\pi^d_i)\\\\\n",
    "    & \\pi^d_i \\sim \\text{Beta}(a_i, b_i)\n",
    "\\end{align}\n",
    "\n",
    "We use EB to estimate $a_i, b_i$, borrowing information from $n^d_i$ across all documents. This is of course over-simplifying, but we hope: shrinkage effect on anchor words are small, as there is lots of variation; shrinkage effect on background words are strong, as their frequency don't vary much.  (We can think of this as a test whether a word changes frequency across documents). Then hopefully we can see most words have almost constant $\\pi_i^d$; as a result in after transforming to rows of $\\bar{C}$ they should be almost the same. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import NMF\n",
    "from scipy.stats import betabinom\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "script_dir = \"../\"\n",
    "sys.path.append(os.path.abspath(script_dir))\n",
    "from file2 import *\n",
    "from factorize import *\n",
    "from smallsim_functions4 import *\n",
    "from misc import *\n",
    "from sinkhorn import * \n",
    "datadir = \"../../../gdrive/github_data/pyJSMF-RAW-data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## n = (n_1, ..., n_D) is total word counts in D documents\n",
    "## x = (x_1, ..., x_D) is counts for one word in D documents\n",
    "## return pi_pos\n",
    "def betabinom_shrinkage(n, x, ab = np.array([1,1])):\n",
    "    ab = mle_bb(n, x, ab)\n",
    "    mu, var =  beta_mean_var(ab[0], ab[1])\n",
    "    \n",
    "    return pos_beta(n, x, ab), ab, mu, var\n",
    "\n",
    "def obj_bb(ab_root, n, x):\n",
    "    ab = ab_root**2\n",
    "    return -betabinom.logpmf(x, n, ab[0], ab[1], loc=0).sum()\n",
    "\n",
    "def mle_bb(n, x, ab):\n",
    "    res = minimize(obj_bb, np.sqrt(ab), args=(n, x), method='BFGS')\n",
    "    \n",
    "    return res.x**2\n",
    "\n",
    "def pos_beta(n, x, ab):\n",
    "    \n",
    "    return (ab[0] + x) / (ab.sum() + n)\n",
    "\n",
    "def beta_mean_var(a, b):\n",
    "    mu = a / (a + b)\n",
    "    var = a*b/((a + b + 1) * (a+b)**2)\n",
    "    \n",
    "    return mu, var\n",
    "\n",
    "\n",
    "# n = np.random.poisson(50, 200)\n",
    "# a = b = 0.5\n",
    "# x = betabinom.rvs(n, a, b)\n",
    "\n",
    "# ab = np.array([0.5, 0.5])\n",
    "# res = mle_bb(n, x, ab)\n",
    "# print(res)\n",
    "# pi_pos, ab, mu, var = betabinom_shrinkage(n, x)\n",
    "# plt.hist(pi_pos)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 1000\n",
    "# p = 1200 ## make this larger to increase difficulty\n",
    "# n_top = 20 ## number of top words in each topic\n",
    "# k = 3\n",
    "# doc_len = 80\n",
    "# np.random.seed(123)\n",
    "\n",
    "# X, Atrue, Ftrue, p0, Ltrue = smallsim_independent(n = n, p = p, k = k, doc_len = doc_len, \n",
    "#                                            n_top = n_top, returnL=True)\n",
    "# w_idx = np.where(X.sum(axis = 0) > 0)[0]\n",
    "# X = X[:,w_idx]\n",
    "\n",
    "# Pi_true = Ltrue @ Ftrue.T\n",
    "\n",
    "# Pi = np.empty([X.shape[0], X.shape[1]])\n",
    "# AB = np.empty([4, X.shape[1]])\n",
    "# N = X.sum(axis = 1)\n",
    "# for i in range(X.shape[1]):\n",
    "#     Pi[:, i], ab, mu, var = betabinom_shrinkage(N, X[:, i])\n",
    "#     AB[:, i] = ab[0], ab[1], mu, var\n",
    "\n",
    "# out = {\"X\":X, \"Atrue\":Atrue, \"Ftrue\":Ftrue, \"Ltrue\":Ltrue, \n",
    "#        \"Pi_true\":Pi_true, \"Pi\":Pi, \"AB\":AB}\n",
    "# file = open('improve_C.pkl', 'wb')\n",
    "# pickle.dump(out, file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datadir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fc755103ebd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{datadir}/improve_C.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datadir' is not defined"
     ]
    }
   ],
   "source": [
    "file = open(f'{datadir}/improve_C.pkl', 'rb')\n",
    "out = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "\n",
    "X = out[\"X\"]\n",
    "Ftrue = out[\"Ftrue\"]\n",
    "Ltrue = out[\"Ltrue\"]\n",
    "Pi_true = out[\"Pi_true\"]\n",
    "Pi = out[\"Pi\"]\n",
    "AB = out[\"AB\"]\n",
    "k = 3 ## forgot to store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "Clong, _, _ = X2C(sparse.coo_matrix(X))\n",
    "Slong_fit, Blong_fit, Along_fit, _, _, _, _, Clong = factorizeC(Clong, K=k, rectifier='AP', optimizer='activeSet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Slong_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.sum(axis = 1)\n",
    "\n",
    "idx = 12\n",
    "pi , ab, mu, var = Pi[:, idx], AB[:2, idx], AB[2, idx], AB[3, idx]\n",
    "\n",
    "print(ab)\n",
    "print([mu, var])\n",
    "\n",
    "plt.scatter(X[:, idx]/N, pi)\n",
    "plt.ylabel(\"posterior\")\n",
    "plt.xlabel(\"MLE\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(Pi_true[:, idx], pi)\n",
    "plt.ylabel(\"posterior\")\n",
    "plt.xlabel(\"truth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 537].sum() ## much smaller than average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 537\n",
    "pi , ab, mu, var = Pi[:, idx], AB[:2, idx], AB[2, idx], AB[3, idx]\n",
    "\n",
    "print(ab)\n",
    "print([mu, var])\n",
    "\n",
    "plt.scatter(X[:, idx]/N, pi)\n",
    "plt.ylabel(\"posterior\")\n",
    "plt.xlabel(\"MLE\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(Pi_true[:, idx], pi)\n",
    "plt.ylabel(\"posterior\")\n",
    "plt.xlabel(\"truth\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_word = X.sum(axis = 0) / X.sum()\n",
    "weights_word_est = Pi.mean(axis = 0)\n",
    "plt.scatter(weights_word, weights_word_est)\n",
    "xpoints = ypoints = plt.xlim()\n",
    "plt.plot(xpoints, ypoints, linestyle='--', color='k', lw=3, scalex=False, scaley=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directly recoverS from $\\Pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi_true_scaled = Pi_true / Pi_true.sum(axis = 0)[None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "Strue_fit, _,  _ = findS(Pi_true.T, 3) ## even forgot to scale right... still answer is correct\n",
    "print(Strue_fit)\n",
    "\n",
    "Strue_fit, _,  _ = findS(Pi_true_scaled.T, 3) ## even forgot to scale right... still answer is correct\n",
    "print(Strue_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi_scaled = Pi / Pi.sum(axis = 0)[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S, _,  _ = findS(Pi_scaled.T, 3)\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi_scaled_pc = Cbar_proj(Pi_scaled.T)\n",
    "\n",
    "Pi_scaled_pc[:, 0].max()\n",
    "idx = np.argmax(Pi_scaled_pc[:, 0])\n",
    "print(Pi_scaled_pc[idx, 0])\n",
    "print((Pi_scaled_pc[:, 0] > 0.005).sum())\n",
    "print(X[:, idx].sum()) ## very very rare word that should be removed\n",
    "print(AB[:, idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi2 = np.delete(Pi, idx, axis=1)\n",
    "Pi2_scaled = Pi2 / Pi2.sum(axis = 0)[None, :]\n",
    "\n",
    "S2, _,  _ = findS(Pi2_scaled.T, 3)\n",
    "print(S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_dim = [0,1]\n",
    "vis_extremal_pca(Cbar_proj(Pi2_scaled.T), S2, which_dim)\n",
    "vis_extremal_pca(Cbar_proj(Pi2_scaled.T), Strue_fit, which_dim)\n",
    "vis_extremal_pca(Cbar_proj(Pi_true_scaled.T), Strue_fit, which_dim)\n",
    "\n",
    "\n",
    "Pi_naive = X / X.sum(axis = 1)[:, None]\n",
    "vis_extremal_pca(Cbar_proj(Pi_naive.T), Strue_fit, which_dim)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Still more outliers to remove\n",
    "\n",
    "* Pi's structure is no longer the right convex hull... Naive Pi also won't work... some very baad extremal points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Form C and recoverS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = Pi.shape\n",
    "Chat = (Pi.T @ Pi)/n\n",
    "Chat_bar = Chat / Chat.sum(axis = 1)[:, None]\n",
    "\n",
    "## construct C from Pi true\n",
    "## ad hoc when filling in diagonal elements\n",
    "Chat0 = (Pi_true.T @ Pi_true)/n\n",
    "Chat0_bar = Chat0 / Chat0.sum(axis = 1)[:, None]\n",
    "\n",
    "S0, _,  _ = findS(Chat0_bar, 3)\n",
    "print(S0)\n",
    "\n",
    "S, _,  _ = findS(Chat_bar, 3)\n",
    "print(S)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_dim = [0,1]\n",
    "vis_extremal_pca(Cbar_proj(Chat), S, which_dim)\n",
    "\n",
    "vis_extremal_pca(Cbar_proj(Chat0), S, which_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It finds the right anchor words!! But caution here the geometry is quite different (shrunk too hard; like a variable selection step)... the advantage of directly working on $\\Pi$ is that outliers are gone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
